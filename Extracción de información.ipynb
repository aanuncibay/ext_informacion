{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "30 de junio de 2020\n",
    "\n",
    "Extracción de Información de un corpus histórico\n",
    "\n",
    "@author: Antonio Anuncibay Zapata\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera parte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las bibliotecas necesarias\n",
    "\n",
    "import nltk\n",
    "import nltk.chunk, nltk.tag\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
    "from nltk import ChunkParserI\n",
    "from nltk.tree import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el corpus hipótetico del histórico de pedidos de la compañía \n",
    "\n",
    "corpus = [\"me puede enviar cuatro bocadillos variados\", \"quisiera tres perros calientes\", \"seis pasteles de limón\", \"Por favor, me puedes enviar una tarta y cinco pastelitos\", \"para enviar, ocho piezas de pollo variadas\", \"Me pones un plato de pasta\", \"siete ensaladas\", \"Quisiera ordenar cuatro bocadillos con chorizo\", \"mi orden sería pescado con patatas\", \"quisiera dos pasteles de nata\", \"quiero una sopa de cebolla\", \"dos carnes\", \"quisiera pedir tres pizzas de hongos y dos shawarma\", \"me gustaría comer bocadillo de costillas\", \"por favor, un refresco y cinco patatas fritas\", \"tengo antojos de un pastel de chocolate\", \"para comer dos raciones de albondigas\", \"quiero llevar tres torrijas\", \"¿Me sirves un pincho de tortilla, por favor?\", \"quiero una pizza margherita\", \"por favor, dos hamburguesas\", \"nueve donus\", \"podría tener cuatro tortillas\", \"nueve pasteles\", \"gazpacho y bocadillos\", \"6 empanadas\", \"cuatro raciones de croquetas\",\"una tabla de quesos\", \"ocho empanadas\", \"me gustaría comer una ensalada\", \"pasta con carne\", \"quisiera ordenar tres chuletas y una ensalada\", \"cinco patatas fritas\", \"nueve pescados\", \"alubias, pan y cuatro bocadillos de jamón\", \"nueve ensaladas cesar\", \"dos gazpachos\", \"paella y pan\",  \"me gustaría comer bocadillo de costillas\", \"cinco pollos con patatas\" ,\"quisiera pedir tres pizzas de queso y dos bocadillos de jamon\", \"me gustaría comer bocadillo de anchoa con refresco grande\", \"un agua mineral y cinco croquetas de hongos\", \"una tortilla y cuatro hamburguesas\", \"tres pinchos de queso con jamon y dos pizzas\", \"cinco porciones de pastel\", \"tres paellas\", \"siete tortillas y pan con ajo\", \"arroz a la marinera\", \"una ración de rabas\"]\n",
    "\n",
    "#len(corpus) #se preparó un corpus con 50 ordenes diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las frases anotadas del Corpus CESS en español\n",
    "\n",
    "sents = cess_esp.tagged_sents()\n",
    "\n",
    "#print(\"sents\")   #se puede comprobar las frases cargadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El primer paso estaría en decidir con cual de los taggers es el más adecuado  \n",
    "#para el etiquetado inicial del corpus de trabajo de la empresa, \n",
    "#haciendo un entrenamiento del corpus CESS\n",
    "\n",
    "#Cargamos el conjunto de frases anotadas en dos conjuntos: \n",
    "##train: 90% de las frases del conjunto\n",
    "##test: 10% restante \n",
    "\n",
    "sents_train = []\n",
    "sents_test = []\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    if i % 10:\n",
    "        sents_train.append(sents[i])\n",
    "    else:\n",
    "        sents_test.append(sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Porcentaje de aciertos\n",
      " \n",
      "\n",
      " *Unigramas:  88 %\n",
      " \n",
      "\n",
      " *Bigramas: 89 %\n",
      " \n",
      "\n",
      " *Trigramas: 89 %\n",
      " \n",
      "\n",
      " *HMMs: 90 %\n"
     ]
    }
   ],
   "source": [
    "#Se establecen los cuatro taggers para conocer su despeño\n",
    "\n",
    "unigram_tagger = UnigramTagger(sents_train)\n",
    "bigram_tagger = BigramTagger(sents_train, backoff = unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(sents_train, backoff = bigram_tagger)\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(sents_train)\n",
    "\n",
    "print(\"  Porcentaje de aciertos\")\n",
    "print(\" \\n\\n *Unigramas: \", round(unigram_tagger.evaluate(sents_test)*100), \"%\")\n",
    "print(\" \\n\\n *Bigramas:\", round(bigram_tagger.evaluate(sents_test)*100), \"%\")\n",
    "print(\" \\n\\n *Trigramas:\", round(trigram_tagger.evaluate(sents_test)*100), \"%\")\n",
    "print(\" \\n\\n *HMMs:\", round(hmm_tagger.evaluate(sents_test)*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusión: \n",
    "\n",
    "    * El que presenta mejor porcentaje de aciertos es el modelo de Hidden Markov; por tanto, se decide elegir este \n",
    "    para realizar el proceso de etiquetado del corpus histórico de trabajo de la empresa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda Parte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación del cuerpo de trabajo\n",
    "\n",
    "* Para realizar el proceso de Extracción de Información primero se debe realizar el etiquetado\n",
    "del corpus histórico de la empresa, para luego aplicarle el proceso de RegexParser con la información a extraer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el corpus entero de frases anotadas CESS en el modelo de Hidden Markov\n",
    "\n",
    "Esp_POS_tagger = nltk.HiddenMarkovModelTagger.train(sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea una gramática para extraer la información según la información morfológica \n",
    "#del modelo de etiquetado EAGLE\n",
    "\n",
    "\n",
    "\n",
    "grammar = r\"\"\"\n",
    "\n",
    "   comida:{<ncfs000|ncms000|ncmp000|rg>?<sps00>*<ncfs000|ncfp000|ncms000|ncmp000|da0fs0>}  ##patrón para frases compuestas, como\n",
    "                                                                                            #bocadillo con chorizo\n",
    "          {<ncfs000|ncms000|ncfp000>?<aq0fs0|aq0fp0|da0fs0|ncms000>}  #patrón para frases compuestas como patatas fritas\n",
    "          {<ncms000>}                                                #patrón para definir nombres o sustantivos\n",
    "          {<ncfs000>}\n",
    "          {<ncfp000>}\n",
    "          {<ncmp000>}\n",
    "          {<da0fs0>}\n",
    "          {<ncfp000>}\n",
    "          {<rg>}\n",
    "\n",
    "   cantidad:{<di0ms0>}  ##patrón para definir las cantidades, el POS etiqueta en estas frases u oraciones\n",
    "                           #las cantidades como determinantes y no como numerales, de igual manera\n",
    "                           #se incluye la etiqueta EAGLE para ambos\n",
    "            {<di0fs0>}\n",
    "            {<dn0cp0>}\n",
    "            {<MCMP00>}\n",
    "            {<MCFS00>}\n",
    "            {<MCFP00>}\n",
    "            {<MCFS00>}\n",
    "            {<MCCP00>}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se declara una función para convertir las cantidades \n",
    "#en números aunque según el enunciado del ejercicio no se consideraba necesario. \n",
    "\n",
    "def clasificar_cantidad(cantidad):\n",
    "\n",
    "    \n",
    "    if (cantidad == \"un\" or cantidad == \"una\" or cantidad == 1) : \n",
    "        Cant_num = 1\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"dos\" or cantidad == 2): \n",
    "        Cant_num = 2\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"tres\" or cantidad == 3): \n",
    "        Cant_num = 3\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"cuatro\" or cantidad == 4): \n",
    "        Cant_num = 4\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"cinco\" or cantidad == 5): \n",
    "        Cant_num = 5\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"seis\" or cantidad == 6): \n",
    "        Cant_num = 6\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"siete\" or cantidad == 7): \n",
    "        Cant_num = 7\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"ocho\" or cantidad == 8): \n",
    "        Cant_num = 8\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"nueve\" or cantidad == 9): \n",
    "        Cant_num = 9\n",
    "        return(Cant_num)\n",
    "    if (cantidad == \"diez\" or cantidad == 10): \n",
    "        Cant_num = 10\n",
    "        return(Cant_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Función de separación de \"cantidades\" y \"comidas\" de\n",
    "#del resultado del proceso de los diferentes procesos de extracción\n",
    "\n",
    "def clasificar_rp(tree):\n",
    "    \n",
    "    #switches booleanos para la separación de los pedidos sin cantidad (null) a los pedidos con cantidad\n",
    "    cant_1 = False  \n",
    "    cant_2 = False\n",
    "    cant_3 = False\n",
    "    \n",
    "    for subtree in tree.subtrees(): #extracción de la información enviada del RegexParser en forma de rama/árbol\n",
    "            \n",
    "        if subtree.label() == \"comida\":          \n",
    "            cant_1 = True \n",
    "            comida = subtree.leaves()\n",
    "            lista_comida = []\n",
    "            for a,b in comida:               \n",
    "                lista_comida.append(a)           \n",
    "            print(\"\\n\\n Comida:\", \" \".join(lista_comida)) \n",
    "\n",
    "        if subtree.label() == \"cantidad\":\n",
    "            cant_2 = True\n",
    "            lista_cantidad = []\n",
    "            cantidad = subtree.leaves()\n",
    "            for a,b in cantidad:\n",
    "                lista_cantidad.append(a)               \n",
    "            cantidad = \" \".join(lista_cantidad)            \n",
    "            num = clasificar_cantidad(cantidad)           \n",
    "            print(\"\\n\\n Cantidad(es):\", num)\n",
    "        \n",
    "        if (cant_1 == True and cant_2 == False and cant_3 == False):     \n",
    "            cant_3 == True          \n",
    "            print(\"Cantidad(es): 1\")\n",
    "                         \n",
    "    print(\"\\n\\n ... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Frase # 1\n",
      "\n",
      "\n",
      " ***Orden: mi orden sería pescado con patatas\n",
      "\n",
      "\n",
      " a. Texto tokenizado:\n",
      "['mi', 'orden', 'sería', 'pescado', 'con', 'patatas']\n",
      "\n",
      "\n",
      " b. Texto etiquetado:\n",
      "[('mi', 'dp1css'), ('orden', 'ncms000'), ('sería', 'spcms'), ('pescado', 'ncms000'), ('con', 'sps00'), ('patatas', 'ncfp000')]\n",
      "\n",
      "\n",
      "  c. Extracción de información con RegexParser:\n",
      "c.1. Resultado RegexParser:\n",
      "(S\n",
      "  mi/dp1css\n",
      "  (comida orden/ncms000)\n",
      "  sería/spcms\n",
      "  (comida pescado/ncms000 con/sps00 patatas/ncfp000))\n",
      "\n",
      "\n",
      " Comida: orden\n",
      "Cantidad(es): 1\n",
      "\n",
      "\n",
      " Comida: pescado con patatas\n",
      "Cantidad(es): 1\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " Frase # 2\n",
      "\n",
      "\n",
      " ***Orden: quisiera pedir tres pizzas de queso y dos bocadillos de jamon\n",
      "\n",
      "\n",
      " a. Texto tokenizado:\n",
      "['quisiera', 'pedir', 'tres', 'pizzas', 'de', 'queso', 'y', 'dos', 'bocadillos', 'de', 'jamon']\n",
      "\n",
      "\n",
      " b. Texto etiquetado:\n",
      "[('quisiera', 'sps00'), ('pedir', 'vmn0000'), ('tres', 'dn0cp0'), ('pizzas', 'ncmp000'), ('de', 'sps00'), ('queso', 'np0000l'), ('y', 'cc'), ('dos', 'dn0cp0'), ('bocadillos', 'ncmp000'), ('de', 'sps00'), ('jamon', 'da0fs0')]\n",
      "\n",
      "\n",
      "  c. Extracción de información con RegexParser:\n",
      "c.1. Resultado RegexParser:\n",
      "(S\n",
      "  quisiera/sps00\n",
      "  pedir/vmn0000\n",
      "  (cantidad tres/dn0cp0)\n",
      "  (comida pizzas/ncmp000)\n",
      "  de/sps00\n",
      "  queso/np0000l\n",
      "  y/cc\n",
      "  (cantidad dos/dn0cp0)\n",
      "  (comida bocadillos/ncmp000 de/sps00 jamon/da0fs0))\n",
      "\n",
      "\n",
      " Cantidad(es): 3\n",
      "\n",
      "\n",
      " Comida: pizzas\n",
      "\n",
      "\n",
      " Cantidad(es): 2\n",
      "\n",
      "\n",
      " Comida: bocadillos de jamon\n",
      "\n",
      "\n",
      " ... \n"
     ]
    }
   ],
   "source": [
    "#Extracción de información basado en RegexParser\n",
    "\n",
    "def __init__():\n",
    "\n",
    "    random.shuffle(corpus) # a manera de ejemplo se hace una reorganización al azar del cuerpo de trabajo\n",
    "                           #para imprimir de manera aleatoria los primeros cinco llamados de la extracción del cuerpo y no las 50 ordenes \n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for sent in corpus[:2]:\n",
    "        \n",
    "        j = j + 1\n",
    "        print(\"\\n\\n Frase #\", j)\n",
    "       \n",
    "        print(\"\\n\\n ***Orden:\", sent)\n",
    "        \n",
    "        sent_tokens = nltk.word_tokenize(sent) #tokenización del texto\n",
    "        print(\"\\n\\n a. Texto tokenizado:\")\n",
    "        print(sent_tokens)\n",
    "\n",
    "        sent_tagged = Esp_POS_tagger.tag(sent_tokens) #proceso de etiquetado\n",
    "        print(\"\\n\\n b. Texto etiquetado:\")\n",
    "        print(sent_tagged)\n",
    "\n",
    "        sent_parser = rp.parse(sent_tagged) #aplicación del análisis según el \"grammar\" creado de patrones\n",
    "        print(\"\\n\\n  c. Extracción de información con RegexParser:\")\n",
    "        print(\"c.1. Resultado RegexParser:\")\n",
    "        print(sent_parser)\n",
    "        clasf = clasificar_rp(sent_parser) #se envía a la función de clasficación\n",
    " \n",
    "__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusión: \n",
    "     * Una vez que se comprueba el funcionamiento del RegexParser para la extracción de información\n",
    "     se usa para la preparación del corpus con etiquetado IOB para trabajar con \n",
    "     los modelos Unigram, Bigram y Naive Bayes\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda Parte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de Información del corpus con los modelos Unigram, Bigram y Naive Bayes basados en el etiquetado IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparación del corpus histórico con etiquetas IOB basados en el RegexParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación del corpus histórico con etiquetado IOB\n",
    "corpus_IOB = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sentence_tagged = Esp_POS_tagger.tag(sentence_tokens)\n",
    "    sentence_parser = rp.parse(sentence_tagged)\n",
    "    chunked_IOB = tree2conlltags(sentence_parser)\n",
    "    corpus_IOB.append(conlltags2tree(chunked_IOB))\n",
    "\n",
    "#Impresión de muestra de etiquetado IOB\n",
    "#random.shuffle(corpus_IOB)\n",
    "#print(\"\\n\\n\", corpus_IOB[:5]) \n",
    "\n",
    "#Corpus de prueba para la comprobaciónn del funcionamiento de los otros parser.\n",
    "corpus_parser = []\n",
    "for sent in corpus:\n",
    "    sent_tokens = nltk.word_tokenize(sent)\n",
    "    sent_tagged = Esp_POS_tagger.tag(sent_tokens)\n",
    "    corpus_parser.append(sent_tagged)\n",
    "\n",
    "#Impresión de muestra\n",
    "#random.shuffle(corpus_parser)\n",
    "#print(\"\\n\\n\", corpus_parser[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el Corpus IOB etiquetado en dos conjuntos: \n",
    "##train: 90% de las frases del conjunto\n",
    "##test: 10% restante \n",
    "\n",
    "train_sents = []\n",
    "test_sents = []\n",
    "\n",
    "for i in range(len(corpus_IOB)):\n",
    "    if i % 10:\n",
    "        train_sents.append(corpus_IOB[i])\n",
    "    else:\n",
    "        test_sents.append(corpus_IOB[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Anotación: los tres modelos a ser usados trabajan de manera bastante parecida. \n",
    "Por un lado tienen una función en la cual separan el etiquetado IOB del corpus presentado (entrenamiento), para luego entrenar el modelo respectivo y volver a construir la frase. Así al presentarse el corpus de prueba usa el entrenamiento del modelo para comprobar su desempeño. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extracción de Información con UnigramParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Unigram_TAG(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return(nltk.chunk.conlltags2tree(conlltags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ChunkParse score:\n",
      "    IOB Accuracy:  91.7%%\n",
      "    Precision:     75.0%%\n",
      "    Recall:        90.0%%\n",
      "    F-Measure:     81.8%%\n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('nueve', 'dn0cp0'), ('pasteles', 'ncmp000')]\n",
      "\n",
      "\n",
      " Cantidad(es): 9\n",
      "\n",
      "\n",
      " Comida: pasteles\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('Quisiera', 'da0mp0'), ('ordenar', 'ao0mp0'), ('cuatro', 'dn0cp0'), ('bocadillos', 'ncmp000'), ('con', 'sps00'), ('chorizo', 'da0fs0')]\n",
      "\n",
      "\n",
      " Cantidad(es): 4\n",
      "\n",
      "\n",
      " Comida: bocadillos con chorizo\n",
      "\n",
      "\n",
      " ... \n"
     ]
    }
   ],
   "source": [
    "UTagger = Unigram_TAG(train_sents)\n",
    "print(\"\\n\\n\", UTagger.evaluate(test_sents))\n",
    "\n",
    "random.shuffle(corpus_parser)\n",
    "for sent in corpus_parser[:2]:\n",
    "    print(\"\\n\\n Texto de prueba:\")\n",
    "    print(\"*Anotado:\", sent)\n",
    "    clasf = UTagger.parse(sent)\n",
    "    clasificar_rp(clasf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extracción de Información con Bigram Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BigramTagger(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (words, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                    in zip(sentence, chunktags)] \n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ChunkParse score:\n",
      "    IOB Accuracy:  83.3%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:        80.0%%\n",
      "    F-Measure:     88.9%%\n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('nueve', 'dn0cp0'), ('pasteles', 'ncmp000')]\n",
      "\n",
      "\n",
      " Cantidad(es): 9\n",
      "\n",
      "\n",
      " Comida: pasteles\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('tres', 'dn0cp0'), ('pinchos', 'ncmp000'), ('de', 'sps00'), ('queso', 'vmn0000'), ('con', 'sps00'), ('jamon', 'np0000l'), ('y', 'cc'), ('dos', 'dn0cp0'), ('pizzas', 'ncmp000')]\n",
      "\n",
      "\n",
      " Cantidad(es): 3\n",
      "\n",
      "\n",
      " Comida: pinchos de\n",
      "\n",
      "\n",
      " ... \n"
     ]
    }
   ],
   "source": [
    "Btagger = BigramTagger(train_sents)\n",
    "print(\"\\n\\n\", Btagger.evaluate(test_sents))\n",
    "\n",
    "random.shuffle(corpus_parser)\n",
    "for sent in corpus_parser[:2]:\n",
    "    print(\"\\n\\n Texto de prueba:\")\n",
    "    print(\"*Anotado:\", sent)\n",
    "    clasf = Btagger.parse(sent)\n",
    "    clasificar_rp(clasf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extracción de Información con Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NBTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):        \n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)       \n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):       \n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)            \n",
    "        return zip(sentence, history)\n",
    "\n",
    "class NBChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = NBTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):       \n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  79.2%%\n",
      "    Precision:     60.0%%\n",
      "    Recall:        60.0%%\n",
      "    F-Measure:     60.0%%\n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('seis', 'dn0cp0'), ('pasteles', 'ncmp000'), ('de', 'sps00'), ('limón', 'da0fs0')]\n",
      "\n",
      "\n",
      " Cantidad(es): 6\n",
      "\n",
      "\n",
      " Comida: pasteles de limón\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " Texto de prueba:\n",
      "*Anotado: [('nueve', 'dn0cp0'), ('pescados', 'ncmp000')]\n",
      "\n",
      "\n",
      " Cantidad(es): 9\n",
      "\n",
      "\n",
      " Comida: pescados\n",
      "\n",
      "\n",
      " ... \n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {\"pos\": pos, \"word\" : word, \"prevpos\": prevpos}\n",
    "\n",
    "NB_Model = NBChunker(train_sents)\n",
    "print(NB_Model.evaluate(test_sents))\n",
    "\n",
    "random.shuffle(corpus_parser)\n",
    "for sent in corpus_parser[:2]:\n",
    "    print(\"\\n\\n Texto de prueba:\")\n",
    "    print(\"*Anotado:\", sent)\n",
    "    clasf = NB_Model.parse(sent)\n",
    "    clasificar_rp(clasf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Conclusión: \n",
    "    * Aunque el Unigram y el Naive tienen un acierto mayor, el Naive tiene una precisión y Recall de casi un 12% mayor en promedio; viéndose reflejada en \"comida\" con frases compuestas como por ejemplo: plato de pasta, bocadillo de chorizo.\n",
    "    * Igualmente, el Bigram al tomar en cuenta dos palabras para su extración o clasificación tiene una precisión mucho mayor que el Unigram o Naive, aunque su cantidad de aciertos se vea con un porcentaje menor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte final: Procesamiento de orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de prueba en donde el usuario puede ingresar un pedido y chequear el resultado según \n",
    "los distintos modelos de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_orden(orden):\n",
    "    \n",
    "    print(\"\\n\\n Texto a ser procesado:\", orden)\n",
    "    sent_tokens = nltk.word_tokenize(orden)\n",
    "    sent_tagged = Esp_POS_tagger.tag(sent_tokens)\n",
    "\n",
    "    print(\"\\n\\n Clasificación de la orden:\")\n",
    "    \n",
    "    print(\"\\n\\n 1. RegexParser:\")\n",
    "    sent_parser = rp.parse(sent_tagged)\n",
    "    clasificar_rp(sent_parser)\n",
    "    \n",
    "    ##unigram\n",
    "    print(\"\\n\\n 2. Unigram:\")\n",
    "    clasf_unigram = UTagger.parse(sent_tagged)\n",
    "    clasificar_rp(clasf_unigram)\n",
    "\n",
    "    ##bigram\n",
    "    print(\"\\n\\n 3. Bigram:\")\n",
    "    clasf_bigram = Btagger.parse(sent_tagged)\n",
    "    clasificar_rp(clasf_bigram)\n",
    "\n",
    "    ##NaiveBayes\n",
    "    print(\"\\n\\n 4. Naives Bayes:\")\n",
    "    clasf_NB = NB_Model.parse(sent_tagged)\n",
    "    clasificar_rp(clasf_NB)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quisiera pedir dos bocadillos de jamon, tres tortillas y seis cervezas\n",
      "\n",
      "\n",
      " Texto a ser procesado: Quisiera pedir dos bocadillos de jamon, tres tortillas y seis cervezas\n",
      "\n",
      "\n",
      " Clasificación de la orden:\n",
      "\n",
      "\n",
      " 1. RegexParser:\n",
      "\n",
      "\n",
      " Cantidad(es): 2\n",
      "\n",
      "\n",
      " Comida: bocadillos\n",
      "\n",
      "\n",
      " Cantidad(es): 3\n",
      "\n",
      "\n",
      " Comida: tortillas\n",
      "\n",
      "\n",
      " Cantidad(es): 6\n",
      "\n",
      "\n",
      " Comida: cervezas\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " 2. Unigram:\n",
      "\n",
      "\n",
      " Comida: Quisiera\n",
      "Cantidad(es): 1\n",
      "\n",
      "\n",
      " Cantidad(es): 2\n",
      "\n",
      "\n",
      " Comida: bocadillos de\n",
      "\n",
      "\n",
      " Cantidad(es): 3\n",
      "\n",
      "\n",
      " Comida: tortillas\n",
      "\n",
      "\n",
      " Cantidad(es): 6\n",
      "\n",
      "\n",
      " Comida: cervezas\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " 3. Bigram:\n",
      "\n",
      "\n",
      " Cantidad(es): 2\n",
      "\n",
      "\n",
      " Comida: bocadillos de\n",
      "\n",
      "\n",
      " ... \n",
      "\n",
      "\n",
      " 4. Naives Bayes:\n",
      "\n",
      "\n",
      " Cantidad(es): 2\n",
      "\n",
      "\n",
      " Comida: bocadillos de jamon\n",
      "\n",
      "\n",
      " Cantidad(es): 3\n",
      "\n",
      "\n",
      " Comida: tortillas\n",
      "\n",
      "\n",
      " Cantidad(es): 6\n",
      "\n",
      "\n",
      " Comida: cervezas\n",
      "\n",
      "\n",
      " ... \n"
     ]
    }
   ],
   "source": [
    "def pedir_comida(): \n",
    "\n",
    "    orden_raw = input() #pedir al usuario una orden de comida\n",
    "    orden_tag = procesar_orden(orden_raw)\n",
    "    \n",
    "pedir_comida()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
